!---------------------------------------------------------------------------
      subroutine usr_particles_init
      include 'SIZE.cuf'
      include 'TOTAL.cuf'
      include 'CTIMER.cuf'
      include 'CMTDATA.cuf'
      include 'CMTPART.cuf'

      parameter (lr=75,li=10)
      common  /cpartr/ rpart(lr,lpart)
      common  /cparti/ ipart(li,lpart)
      common  /iparti/ n,nr,ni


      common /part_options/ bc_part, two_way, red_interp,time_integ,
     >                 part_force
      integer bc_part,two_way, red_interp,time_integ,part_force(4)
      nr = lr                ! Mandatory for proper striding
      ni = li                ! Mandatory
c     particle options
      time_integ = 0         ! 0 = bdf/ext, 1 = rk3; time integration;
                       ! note that qs force is already in bdf, so
                       ! the only forces to include is the user
                       ! specified force and no others.
      bc_part    = 1         ! 0 = outflow, 1 = periodic; boundary cond.
      two_way    = 0         ! current two way coupling = 1, or off = 0
      red_interp = 0         ! if 0, full interp. If (lx1+1)/2, reduced
                       !  interp, but lx1 must be odd number
                       !    i.e., red_interp = (lx1+1)/2
      part_force(1) = 1      ! user specified force (i.e., gravity)
      part_force(2) = 1      ! quasi-steady force; if bdf, turn off
      part_force(3) = 0      ! undisturbed force
      part_force(4) = 0      ! inviscid unsteady force; if bdf, turn off

      call nekgsync()
      if (nid.eq.1) then
         print *,"Clock Resolution is ",MPI_WTICK(),
     > "Synchronisation ",MPI_WTIME_IS_GLOBAL,"Rank ",nid

c        print *,"Minimal value of dnekclock() is ",
c    >    dnekclock()-dnekclock()
      endif

      particles_init_s = dnekclock()
      call rzero(rpart,lr*lpart)
      call izero(ipart,li*lpart)
      call set_bounds_box
      call set_part_pointers

c      print *,"Size of rpart=",SIZEOF(rpart)

c      print *,"Size of ipart=",SIZEOF(ipart)
      particles_init_e = dnekclock()
      call init_interpolation(red_interp) ! barycentric weight for interpolation
      particles_interp_e = dnekclock()
      call place_particles                ! n initialized here
      place_par_e = dnekclock()
      call move_particles_inproc          ! initialize fp & cr comm handles
      return
      end


!------------------------------------------------------------------------------
      subroutine set_bounds_box
c
c     set domain and element bounds for a box geometry. Notice that
c     this ONLY works with non curved elements.
c
      include 'SIZE.cuf'
      include 'TOTAL.cuf'
      include 'CTIMER.cuf'
      include 'CMTTIMERS.cuf'
      include 'CMTPART.cuf'

      real   xdrange(2,3)
      common /domainrange/ xdrange
      real   xerange(2,3,lelt)
      common /elementrange/ xerange

      ! begin timer
      ptdum(2) = dnekclock()

      if(istep.eq.0.or.istep.eq.1)then
        call domain_size(xdrange(1,1),xdrange(2,1),xdrange(1,2)
     $                  ,xdrange(2,2),xdrange(1,3),xdrange(2,3))
        ntot = lx1*ly1*lz1*nelt
        nxyz = lx1*ly1*lz1
        do ie = 1,nelt
           xerange(1,1,ie) = vlmin(xm1(1,1,1,ie),nxyz)
           xerange(2,1,ie) = vlmax(xm1(1,1,1,ie),nxyz)
           xerange(1,2,ie) = vlmin(ym1(1,1,1,ie),nxyz)
           xerange(2,2,ie) = vlmax(ym1(1,1,1,ie),nxyz)
           xerange(1,3,ie) = vlmin(zm1(1,1,1,ie),nxyz)
           xerange(2,3,ie) = vlmax(zm1(1,1,1,ie),nxyz)
        enddo
      endif

      ! end timer
      pttime(2) = pttime(2) + dnekclock() - ptdum(2)

      return
      end

!-----------------------------------------------------------------------
      subroutine set_part_pointers
      include 'SIZE'
      include 'CMTPART'

      ! begin timer
c      ptdum(4) = dnekclock()

c     ipart pointers ------------------------------------------------
      jrc   = 1 ! Pointer to findpts return code
      jpt   = 2 ! Pointer to findpts return processor id
      je0   = 3 ! Pointer to findpts return element id
      jps   = 4 ! Pointer to proc id for data swap
      jpid1 = 5 ! initial proc number
      jpid2 = 6 ! initial local particle id
      jpid3 = 7 ! initial time step introduced
      jpnn  = 8 ! initial time step introduced
      jpid  = 9 ! initial time step introduced
      jai   = 10 ! Pointer to auxiliary integers

      nai = ni - (jai-1)  ! Number of auxiliary integers
      if (nai.le.0) call exitti('Error in nai:$',ni)

c     rpart pointers ------------------------------------------------
      jr  = 1         ! Pointer to findpts return rst variables
      jd  = jr + 3    ! Pointer to findpts return distance
      jx  = jd + 1    ! Pointer to findpts input x value
      jy  = jx + 1    ! Pointer to findpts input y value
      jz  = jy + 1    ! Pointer to findpts input z value
      jv0 = jz + 1    ! particle velocity at this timestep
      ju0 = jv0 + 3   ! fluid velocity at this time step
      jf0 = ju0 + 3   ! particle total force at this timestep

c     forcing
      ii  = jf0 + 3
      if (part_force(1).ne.0) then ! user specified force
         jfusr = ii
         ii    = ii + 3
      endif
      if (part_force(2).ne.0) then ! quasi-steady force
         jfqs  = ii
         ii    = ii + 3
      endif
      if (part_force(3).ne.0) then ! undisturbed force
         jfun  = ii
         ii    = ii + 3
      endif
      if (part_force(4).ne.0) then ! inviscid unsteady force
         jfiu  = ii
         ii    = ii + 3
      endif

c     other parameters (some may not be used; all at part. location)
      jtaup   = ii          ! particle time scale
      jcd     = jtaup   + 1 ! drag coeff
      jdrhodt = jcd     + 3 ! density material time derivative
      jre     = jdrhodt + 1 ! Relative Reynolds number
      jDuDt   = jre     + 1 ! fluid velocity time derivative
      jtemp   = jDuDt   + 3 ! part. temperature (assume same as fluid)
      jrho    = jtemp   + 1 ! fluid denisty
      jrhop   = jrho    + 1 ! particle material density
      ja      = jrhop   + 1 ! fluid mach number
      jvol    = ja      + 1 ! particle volume
      jvol1   = jvol    + 1 ! particle volume fraction at part. loc.
      jdp     = jvol1   + 1 ! particle diameter
      jgam    = jdp     + 1 ! spread to grid correction
      jspl    = jgam    + 1 ! super particle loading

c     bdf/ext integration
      jx1 = jspl+1 ! Pointer to xyz at t^{n-1}
      jx2 = jx1 +3 ! Pointer to xyz at t^{n-1}
      jx3 = jx2 +3 ! Pointer to xyz at t^{n-1}

      jv1 = jx3+ 3 ! Pointer to particle velocity at t^{n-1}
      jv2 = jv1+ 3 ! Pointer to particle velocity at t^{n-2}
      jv3 = jv2+ 3 ! Pointer to particle velocity at t^{n-3}

      ju1 = jv3+ 3 ! Pointer to fluid velocity at t^{n-1}
      ju2 = ju1+ 3 ! Pointer to fluid velocity at t^{n-2}
      ju3 = ju2+ 3 ! Pointer to fluid velocity at t^{n-3}

      jar = ju3+ 3 ! Pointer to auxiliary reals

      nar = nr - (jar-1)  ! Number of auxiliary reals
      if (nar.le.0) call exitti('Error in nar:$',nr)

c     ghost particle integer pointers -------------------------------
      jgppid1 = 1 ! initial proc number
      jgppid2 = 2 ! initial local particle id
      jgppid3 = 3 ! initial time step introduced
      jgpps   = 4 ! Pointer to proc id for data swap
      jgppt   = 5 ! findpts return processor id
      jgpes   = 6 ! Destination element to be sent to

c     ghost particle real pointers ----------------------------------
      jgpx    = 1 ! ghost particle xloc
      jgpy    = 2 ! ghost particle yloc
      jgpz    = 3 ! ghost particle zloc
      jgpfh   = 4 ! ghost particle hydrodynamic xforce (i+1 > y, i+2 > z)
      jgpvol  = jgpfh+3  ! ghost particle volume
      jgpgam  = jgpvol+1 ! spreading correction (if used)
      jgpspl  = jgpgam+1 ! super particle loading

      ! end timer
c      pttime(4) = pttime(4) + dnekclock() - ptdum(4)

      return
      end

c-----------------------------------------------------------------------
c     interpolation routines
c-----------------------------------------------------------------------
      subroutine init_interpolation
      include 'SIZE'
      include 'INPUT'
      include 'CMTPART'
c
c     calculates the barycentric lagrange weights
c

      ! begin timer
      ptdum(29) = dnekclock()

c     get gll points in all directions
      call zwgll(xgll,wxgll,lx1)
      call zwgll(ygll,wygll,ly1)
      call rone(zgll,lz1)
      if(if3d) call zwgll(zgll,wzgll,lz1)
c     set all weights to ones first
      call rone(wxgll,lx1)
      call rone(wygll,ly1)
      call rone(wzgll,lz1)
c
c     copy for reduced interpolation
      nx1r = lx1
      if (red_interp.gt.0) then
         nx1r = red_interp
         ic = 0
         do j=1,lx1,2
            ic = ic + 1
            xgll(ic) = xgll(j)
            ygll(ic) = ygll(j)
            zgll(ic) = zgll(j)
         enddo
      endif

c     calc x bary weights
      do j=1,nx1r
         do k=1,nx1r
            if (j .NE. k) then
               wxgll(j) = wxgll(j)/(xgll(j) - xgll(k))
            endif
         enddo
      enddo
c     calc y bary weights
      do j=1,nx1r
         do k=1,nx1r
            if (j .NE. k) then
               wygll(j) = wygll(j)/(ygll(j) - ygll(k))
            endif
         enddo
      enddo
c     calc z bary weights
      do j=1,nx1r
         do k=1,nx1r
            if (j .NE. k) then
               wzgll(j) = wzgll(j)/(zgll(j) - zgll(k))
            endif
         enddo
      enddo

      ! end timer
      pttime(29) = pttime(29) + dnekclock() - ptdum(29)

      return
      end

c----------------------------------------------------------------------
c     effeciently move particles between processors routines
c----------------------------------------------------------------------
      subroutine move_particles_inproc
c     Interpolate fluid velocity at current xyz points and move
c     data to the processor that owns the points.
c     Input:    n = number of points on this processor
c     Output:   n = number of points on this processor after the move
c     Code checks for n > llpart and will not move data if there
c     is insufficient room.
      include 'SIZE.cuf'
      include 'TOTAL.cuf'
      include 'CTIMER.cuf'
      include 'CMTPART.cuf'

      common /nekmpi/ mid,mp,nekcomm,nekgroup,nekreal
      common /myparth/ i_fp_hndl, i_cr_hndl

      integer icalld1
      save    icalld1
      data    icalld1 /0/

      logical partl         ! This is a dummy placeholder, used in cr()

      ! begin timer
c      ptdum(35) = dnekclock()

      nl = 0                ! No logicals exchanged

      if (icalld1.eq.0) then
         tolin = 1.e-12
         if (wdsize.eq.4) tolin = 1.e-6
         call intpts_setup  (tolin,i_fp_hndl)
         call crystal_setup (i_cr_hndl,nekcomm,np)
         icalld1 = icalld1 + 1
      endif

      call particles_in_nid_gpu

      call findpts(i_fp_hndl !  stride     !   call findpts( ihndl,
     $           , ifpts(jrc,1),lif        !   $             rcode,1,
     $           , ifpts(jpt,1),lif        !   &             proc,1,
     $           , ifpts(je0,1),lif        !   &             elid,1,
     $           , rfpts(jr ,1),lrf        !   &             rst,ndim,
     $           , rfpts(jd ,1),lrf        !   &             dist,1,
     $           , rfpts(jx ,1),lrf        !   &             pts(    1),1,
     $           , rfpts(jy ,1),lrf        !   &             pts(  n+1),1,
     $           , rfpts(jz ,1),lrf ,nfpts)    !   &             pts(2*n+1),1,n)

      nmax = iglmax(n,1)
      if (nmax.gt.llpart) then
         if (nid.eq.0) write(6,1) nmax,llpart
    1    format('WARNING: Max number of particles:',
     $   i9,'.  Not moving because llpart =',i9,'.')
      else
c        copy rfpts and ifpts back into their repsected positions in rpart and ipart
         call update_findpts_info
c        Move particle info to the processor that owns each particle
c        using crystal router in log P time:

         jps = jpid1-1     ! Pointer to temporary proc id for swapping
         do i=1,n        ! Can't use jpt because it messes up particle info
            ipart(jps,i) = ipart(jpt,i)
         enddo
         call crystal_tuple_transfer(i_cr_hndl,n,llpart
     $              , ipart,ni,partl,nl,rpart,nr,jps)
c        Sort by element number - for improved local-eval performance
         call crystal_tuple_sort    (i_cr_hndl,n
     $              , ipart,ni,partl,nl,rpart,nr,je0,1)
      endif

      ! end timer
c      pttime(35) = pttime(35) + dnekclock() - ptdum(35)

      return
      end

!----------------------------------------------------------------------------
      subroutine particles_in_nid_gpu
      include 'SIZE.cuf'
      include 'TOTAL.cuf'
      include 'CTIMER.cuf'
      include 'CMTPART.cuf'

      real, device :: d_rpart(lr,lpart)
      integer, device :: d_ipart(li,lpart)
      integer, device :: d_fptsmap(lpart)
      real, device :: d_rfpts(lrf,lpart)
      integer, device :: d_ifpts(lif,lpart)
      real, device :: d_xerange(2,3,lelt)

      if(n.gt.0) then
         call particles_in_nid_wrapper(d_fptsmap,d_rfpts,d_ifpts,d_rpart,  &
             d_ipart,d_xerange,nrf,nif,nfpts,nr,ni,n,lpart,nelt,jx,jy,jz,  &
             je0,jrc,jpt,jd,jr,nid)
      endif

      if(nfpts.gt.0) then
         istate = cudaMemcpy(ifpts,d_ifpts,lif*n, &
                        cudaMemcpyDevicetoHost)
         istate = cudaMemcpy(rfpts,d_rfpts,lrf*n, &
                        cudaMemcpyDevicetoHost)
         istate = cudaMemcpy(fptsmap,d_fptsmap,n, &
                        cudaMemcpyDevicetoHost)
      endif

      if(n.gt.0) then
         istate = cudaMemcpy(rpart,d_rpart,nr*n, &
                          cudaMemcpyDevicetoHost)
         istate = cudaMemcpy(ipart,d_ipart,ni*n, &
                          cudaMemcpyDevicetoHost)
	 !print *, 'after copy'
      endif

      return
      end

!------------------------------------------------------------------------------
c----------------------------------------------------------------------
c     particle force routines
c----------------------------------------------------------------------
      subroutine usr_particles_solver
c
c     call routines in ordered way - main solver structure
c
      include 'SIZE.cuf'
      include 'TOTAL.cuf'
      include 'CMTDATA.cuf'
      include 'CMTPART.cuf'

      logical ifinject
      integer icalld
      save    icalld
      data    icalld  /-1/

      ! begin timer
      ptdum(5) = dnekclock()

c     should we inject particles  -  not included for this phase
c      ifinject = .false.
c      if (inject_rate .gt. 0) then
c      if ((mod(istep,inject_rate).eq.0)) then
c         ifinject = .true.
c      endif
c      endif
c----------time
c      if (istep .gt. time_delay) then

c     bdf/ext integration ---------------------------------------------
      if (time_integ .eq. 0) then
         if (icalld .ne. istep) then
            call update_particle_location   ! move outlier particles
            if (ifinject) call place_particles
            call move_particles_inproc      ! update mpi rank
            call interp_props_part_location ! interpolate
            call usr_particles_forces       ! fluid to part. forcing
            call update_vel_and_pos_bdf     ! time integration
            call compute_forces_post_part   ! update forces
            if (two_way.eq.1) then          ! part. to fluid forcing
               call particles_solver_nearest_neighbor ! nn
               call spread_props_grid       ! put particle props on grid
            endif

            icalld = istep
         endif
c     rk3 integration -------------------------------------------------
      elseif (time_integ .eq. 1) then
         if (stage.eq.1) then
            call update_particle_location_gpu   ! move outlier particles
            if (ifinject) call place_particles
            call move_particles_inproc      ! update mpi rank
         endif
         call interp_props_part_location    ! interpolate
         call usr_particles_forces          ! fluid to part. forcing
         call update_vel_and_pos_rk3        ! time integration
         call compute_forces_post_part ! update forces
         if (two_way.eq.1) then             ! part. to fluid forcing
            call particles_solver_nearest_neighbor    ! nn
            call spread_props_grid          ! put particle props on grid
         endif

c     PIEP ------------------------------------------------------------
      elseif (time_integ .eq. 2) then
      endif

c      endif

      ntmp = iglsum(n,1) ! collective communication call to sync up
      ! end timer
      pttime(5) = pttime(5) + dnekclock() - ptdum(5)

      return
      end
c----------------------------------------------------------------------
!----------------------------------------------------------------------
      subroutine update_particle_location_gpu
      include 'SIZE.cuf'
      include 'TOTAL.cuf'
      include 'CTIMER.cuf'
      include 'CMTPART.cuf'

      real, device :: d_rpart(lr,lpart)
      integer, device :: d_ipart(li,lpart)
      real, device :: d_xdrange(2,3)
      integer, device :: d_in_part(llpart)
      integer, device :: d_bc_part(6)    ! Need to allocate this somewhere
      if(n.gt.0) then
         call update_particle_location_wrapper(d_rpart, d_ipart, d_xdrange,
     >         d_in_part, d_bc_part, ndim, nr, ni, n, jx, jx1, jx2, jx3)
         istate = cudaMemcpy(rpart,d_rpart,nr*n, &
                          cudaMemcpyDevicetoHost)
         istate = cudaMemcpy(ipart,d_ipart,ni*n, &
                          cudaMemcpyDevicetoHost)
      endif
      nbc_sum = abs(bc_part(1)) + abs(bc_part(2)) +
     >          abs(bc_part(3)) + abs(bc_part(4)) +
     >          abs(bc_part(5)) + abs(bc_part(6)) ! all periodic, don't search
      if (nbc_sum .gt. 0) then
      ic = 0
c     right now couldnt think of a way to parallelize this memory update operation due to synchronization problem
c     Need to talk to Dr.Tania and Dr.Ranka to see if there is a way to parallelize this
      do i=1,n
         if (in_part(i).eq.0) then
            ic = ic + 1
            if (i .ne. ic) then
               call copy(rpart(1,ic),rpart(1,i),nr)
               call icopy(ipart(1,ic),ipart(1,i),ni)
            endif
         endif
      enddo
      n = ic
      endif

      return
      end

c------------------------------------------------------------------------------
